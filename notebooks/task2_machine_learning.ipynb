{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_task_2.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('../scripts'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from data_processing import clean_data2, feature_engineering2\n",
    "from model_training import train_random_forest, evaluate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25192.PS\\AppData\\Local\\Temp\\ipykernel_5988\\3654497858.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('../data/rossmann-store-sales/train.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('../data/rossmann-store-sales/train.csv')\n",
    "store = pd.read_csv('../data/rossmann-store-sales/store.csv')\n",
    "\n",
    "#print(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\EDUCATION\\Kifiya_AI_Mastery_Program\\week-4\\scripts\\data_processing.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['CompetitionDistance'].fillna(data['CompetitionDistance'].median(), inplace=True)\n",
      "e:\\EDUCATION\\Kifiya_AI_Mastery_Program\\week-4\\scripts\\data_processing.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['PromoInterval'].fillna('NoPromo', inplace=True)\n",
      "e:\\EDUCATION\\Kifiya_AI_Mastery_Program\\week-4\\scripts\\data_processing.py:56: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['CompetitionOpenSinceMonth'].fillna(0, inplace=True)\n",
      "e:\\EDUCATION\\Kifiya_AI_Mastery_Program\\week-4\\scripts\\data_processing.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['CompetitionOpenSinceYear'].fillna(0, inplace=True)\n",
      "e:\\EDUCATION\\Kifiya_AI_Mastery_Program\\week-4\\scripts\\data_processing.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Promo2SinceWeek'].fillna(0, inplace=True)\n",
      "e:\\EDUCATION\\Kifiya_AI_Mastery_Program\\week-4\\scripts\\data_processing.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Promo2SinceYear'].fillna(0, inplace=True)\n",
      "e:\\EDUCATION\\Kifiya_AI_Mastery_Program\\week-4\\scripts\\data_processing.py:65: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data['Date'] = pd.to_datetime(data['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 440.83101676736493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Clean and preprocess data\n",
    "train_cleaned = clean_data2(train, store)\n",
    "train_fe = feature_engineering2(train_cleaned)\n",
    "\n",
    "# Split data and train a RandomForest model\n",
    "X = train_fe.drop(columns=['Sales'])\n",
    "y = train_fe['Sales']\n",
    "\n",
    "model, X_test, y_test = train_random_forest(X, y)\n",
    " \n",
    "# Evaluate the model\n",
    "evaluate_model(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 2.3: Choose a Loss Function\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[1;32m----> 5\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, \u001b[43my_pred\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Absolute Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2.3: Choose a Loss Function\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m\n\u001b[0;32m     23\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     24\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     25\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m     26\u001b[0m ])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Get feature importances (note: this will only work correctly after the model is fitted)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature_importances_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2.4: Post Prediction Analysis\n",
    "\n",
    "#Feature Importance: Extract feature importance from the mode\n",
    "\n",
    "# Assuming you have already loaded your data\n",
    "# Define numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()  # Automatically selects categorical features\n",
    "\n",
    "# Define the preprocessor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),  # Scale numeric features\n",
    "        ('cat', OneHotEncoder(), categorical_features)  # Encode categorical features\n",
    "    ])\n",
    "\n",
    "# Create a new pipeline with preprocessing\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances (note: this will only work correctly after the model is fitted)\n",
    "feature_importances = pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "ohe = pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "feature_names = ohe.get_feature_names_out(input_features=categorical_features)\n",
    "\n",
    "# Combine feature names from both numeric and categorical features\n",
    "all_feature_names = list(numeric_features) + list(feature_names)\n",
    "\n",
    "# Create the DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importances})\n",
    "importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     upper \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(bootstraps, \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lower, upper\n\u001b[1;32m---> 11\u001b[0m lower_ci, upper_ci \u001b[38;5;241m=\u001b[39m bootstrap_ci(\u001b[43my_pred\u001b[49m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence Interval: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlower_ci\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupper_ci\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Confidence Intervals: Use bootstrapping to estimate confidence intervals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap_ci(data, n_bootstraps=1000, alpha=0.05):\n",
    "    bootstraps = np.random.choice(data, (n_bootstraps, len(data)), replace=True).mean(axis=1)\n",
    "    lower = np.percentile(bootstraps, 100 * alpha / 2)\n",
    "    upper = np.percentile(bootstraps, 100 * (1 - alpha / 2))\n",
    "    return lower, upper\n",
    "\n",
    "lower_ci, upper_ci = bootstrap_ci(y_pred)\n",
    "print(f'Confidence Interval: [{lower_ci}, {upper_ci}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_13-01-2025-00-10-07.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the Model\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n",
    "joblib.dump(pipeline, f'model_{timestamp}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w4venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
